---
title: "Power Analysis"
output: html_notebook
---


# A statistical power analysis

Experimentalists and statisticians will often use simulated data to help design an experiment. In these analyses, the investigator generates data that will look like the expected results of an experiment assuming some amount of variance or error. The investigator can then ask how many experimental units would be required to detect a significant effect, assuming the strength of the relationship and error assumed when simulating data. 

We will use this approach to ask whether ANOVA- or regression-design experiments are more powerful in detecting a continuous relationship between two variables. Assume that an independent variable x is linearly related to a dependent variable y with a slope (??1) of 0.4 and a y-intercept (??0) of 10. Imagine that we have 16 experimental units that can be used to test for an effect of x on y. One could randomly distribute the experimental units along a gradient of x (regression design) or one could replicate two levels of x 12 times each and ask whether low or high levels of x generate different levels of y (t-test design). An intermediate approach would be to have 8 replicates of three levels of x or 4 replicates of six levels of x and so on (ANOVA design). Using the relationship between x and y defined by ??1 = 0.4 and ??0 = 10, we can generate a random dataset with 24 observations of y for any experimental design of interest given some level of error in our observations.

We define our error by the standard deviation (??) of normally distributed errors that we can add to our simulated values of y. Because our results will vary a bit from one simulated data set to another, it is a good idea to adopt something called a monte carlo approach where we run multiple (say 10) versions of a power analysis for a given experimental design and ??