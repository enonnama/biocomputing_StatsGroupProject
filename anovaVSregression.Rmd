---
title: "ANOVA and Regression Results"
output: html_document
---

# PART 1 REGRESSION VS ANOVA

## An ANOVA-design and a regression-design experiment

```{r include=FALSE}
rm(list=ls())
setwd("/Users/emilynonnamaker/Box/personalStuff/School/PhD/biocomputingFall2018/biocomputing_StatsGroupProject")
library(ggplot2)
library(arm)
antibiotics <- read.csv("antibiotics.csv")
sugar <- read.csv("sugar.csv")

#playing around - check out the data
boxplot(growth ~ trt, data=antibiotics)
lm <- lm(growth ~ sugar, data=sugar)
aov <- aov(growth ~ trt, data=antibiotics)

# how a normal person would do this
summary(aov)
posthoc <- TukeyHSD(aov, "trt", conf.level=0.95) #a, b, ac, bd. 
posthoc

# Now to do the actual assingment:

# set up your data: 

N=length(antibiotics$growth)
y=antibiotics$growth
x<- antibiotics$trt
antibiotics$x1 <- ifelse(x=="ab1", 1, 0) #dummy variables
antibiotics$x2 <- ifelse(x=="ab2", 1, 0) #dummy variables
antibiotics$x3 <- ifelse(x=="ab3", 1, 0) #dummy variables

#Build null model 
nullmod<-function(p,y){
  B0=p[1]
  sigma=exp(p[2])
  
  expected=B0

null=-sum(dnorm(x=y,mean=expected,sd=sigma, log = TRUE))
return(null)
}

#Build full model
fullmod<-function(p,x,y){
B0=p[1]
B1=p[2]
B2=p[3]
B3=p[4]
sigma=exp(p[5])

expected=B0+B1*x[,3]+B2*x[,4]+B3*x[,5]

full=-sum(dnorm(x=y,mean=expected,sd=sigma,log = TRUE))
return(full)
}

# Check fit
nullguess <- c(1, 2)
fullguess <- c(1, 2, 3, 4, 5)
fitnull=optim(par=nullguess,fn=nullmod,y=antibiotics$growth)
fitfull=optim(par=fullguess,fn=fullmod,x=antibiotics,y=antibiotics$growth)

# Get t.statistic and p value. 
df <- length(fitfull$par) - length(fitnull$par)
t.stat <- 2*(fitnull$value - fitfull$value)
t.stat # 25.69042

1-pchisq(t.stat, df=df)
```

Looks like there's a significant difference between the null and full anova model (t = 25.69, p < .001), suggesting that there are differences between treatments. 

## Graphical visualization!

Using a posthoc Tukey test (see hidden code) we can find significant differences between groups, and add these differences as letters to our visualization (see graph below)

```{r}
p <- ggplot(antibiotics, aes(x=trt, y=growth, fill=trt)) + 
  geom_boxplot()
p + geom_jitter(shape=16, position=position_jitter(0.2), alpha=0.3) + theme_classic() + labs(x = "Treatment", y= "Growth") + annotate("text", x = c(1, 2, 3, 4), y = c(23, 23, 23, 23), label = c("a", "b", "ac", "bd"))

```

## Regression

2) Another student conducted an experiment evaluating the effect of sugar concentration on growth of E. coli in lab cultures. Using the data in sugar.txt, generate a plot that summarizes the results and test for an effect of sugar concentration on growth of E. coli using a regression-design linear model and likelihood ratio test
 
lmnullmod<-function(p,x,y){
  B0=p[1]
  sigma=exp(p[2])
  
  expected=B0

null=-sum(dnorm(x=y,mean=expected,sd=sigma, log = TRUE))
return(null)
}
```

Now build the extension:

```{r}
lmfullmod<-function(p,x,y){
  B0=p[1]
  B1=p[2]
  sigma=exp(p[3])
  
  expected=B0+B1*x

  null=-sum(dnorm(x=y,mean=expected,sd=sigma, log = TRUE))
  return(null)
}
```

Give it a guess:

```{r}
lmnullguess <- c(1, 1)
lmfullguess <- c(1, 2, 3)
lmfitnull=optim(par=lmnullguess,fn=lmnullmod,x=sugar$sugar,y=sugar$growth)
lmfitfull=optim(par=lmfullguess,fn=lmfullmod,x=sugar$sugar,y=sugar$growth)
```

Now to compare:

```{r}

t.stat <- 2*(lmfitnull$value - lmfitfull$value)
t.stat # 39.92512

1-pchisq(t.stat, df=1) # 2.638878e-10
```

Comparing between the null model of no effect of sugar concentration on growth to the model taking this affect into account, we can see that sugar concentration does have a significant affect on growth (t = 39.92512, p < .0001). Looking at the graph, we can tell that sugar concentration has a positive affect, with higher concentrations increasing growth. 

## Visualization!

```{r}
a <- ggplot(data=sugar,aes(x=sugar,y=growth))
a + geom_point() + coord_cartesian() + labs( x = "sugar concentration", y = "growth") + theme_classic() + geom_smooth(method = "lm")
```

# Part 2 Statistical Power Analysis

```{r}

########################
# Put in your models  #
#######################

# Null model

nullmod<- function(p,x,y){
  B0=p[1]
  sigma=exp(p[2])
  expected=B0
  null=-sum(dnorm(x=y,mean=expected,sd=sigma, log = TRUE))
  return(null)
}

# Regression model
lin<-function(p,x,y){
  B0=p[1]
  B1=p[2]
  sigma=exp(p[3])
  expected=B0+B1*x

  lmmod=-sum(dnorm(x=y,mean=expected,sd=sigma, log = TRUE))
  return(lmmod)
}

# T-test model
ttest<-function(p,x,y){
  B0=p[1]
  B1=p[2]
  sigma=exp(p[3])
  x1=x
  expected=B0+B1*x1

  tmod=-sum(dnorm(x=y,mean=expected,sd=sigma, log = TRUE))
  return(tmod)
}

# Anova - four groups
anovamod<-function(p,x,y){
B0=p[1]
B1=p[2]
B2=p[3]
B3=p[4]
sigma=exp(p[5])

expected=B0+B1*x[,1]+B2*x[,2]+B3*x[,3]
full=-sum(dnorm(x=y,mean=expected,sd=sigma,log = TRUE))
return(full)
} #optim-call call dummy

# Now an anova with 8 groups
eightanovamod<-function(p,x,y){
  B0=p[1]
  B1=p[2]
  B2=p[3]
  B3=p[4]
  B4=p[5]
  B5=p[6]
  B6=p[7]
  B7=p[8]
  sigma=exp(p[9])
  expected=B0+B1*x[,1]+B2*x[,2]+B3*x[,3]+B4*x[,4]+B5*x[,5]+B6*x[,6]+B7*x[,7]
  eightanovamod=-sum(dnorm(x=y,mean=expected,sd=sigma, log = TRUE))
  return(eightanovamod)
}

### Now do your for loops!

# Set up an empty data frame in which to store your p-values for each sigma and model.
dfp <- setNames(data.frame(matrix(ncol = 5, nrow = 8)), c("Sigmas", "Regression", "T.test", "Anova",  "Eight.Anova"))

# Set up your vector of sigmas to loop through
sigmas <- c(1,2,4,6,8,12,16,24)

# Write your nested for loop:
for (i in 1:length(sigmas)){ # sigma runs
  # set up your vectors for your p-values from each monte-carlo run. 
  reg.p <- numeric(10) # changes each time you use a new sigma
  t.p <- numeric(10)
  anova.p <- numeric(10)
  eightanova.p <- numeric(10)
  for (j in 1:10){ # monte carlo runs
    x <- runif(24, 0, 50) # build set of x's
    e <- rnorm(24, 0, sd = sigmas[i]) # put in your error
    y <- 10 + 0.4*x + e # write your linear equation
    df <- cbind(x, y) # bind x and y into a data frame
    df <- as.data.frame(df)
    df <- df[order(df$x),] # order for later sorting. 
    
    # dummy coding:
    
    # For t-test
    tdummy <- matrix(0, 24, 1)
    tdummy[13:24,1] <- 1
  
    # For anova
    dummy <- matrix(0, 24, 3)
    dummy[7:12,1] <- 1
    dummy[13:18,2] <-1
    dummy[19:24,3] <-1
    
    #for eight-level anova
    edummy <- matrix(0, 24, 7)
    edummy[4:6,1]<-1
    edummy[7:9,2]<-1
    edummy[10:12,3]<-1
    edummy[13:15,4]<-1
    edummy[16:18,5]<-1
    edummy[19:21,6]<-1
    edummy[22:24,7]<-1
    
    # guesses
    nullmodguess <- c(1,2)
    lmmodguess <- c(1, 2, 3)
    tmodguess<-c(1,2,3,4,5)
    aovguess<- c(1,2,3,4,5)
    eightaovguess<-c(1,2,3,4,5,6,7,8,9)
    
    #optims
    nullfitmod=optim(par=nullmodguess,fn=nullmod,x=df$x,y=df$y)
    lmfitmod=optim(par=lmmodguess,fn=lin,x=df$x,y=df$y)
    tfitmod=optim(par=tmodguess,fn=ttest,x=tdummy,y=df$y)
    anovafitmod=optim(par=aovguess,fn=anovamod,x=dummy,y=df$y)
    eightanovafitmod=optim(par=eightaovguess,fn=eightanovamod,x=edummy,y=df$y)
   
    # degrees of freedom 
     degfreg <- length(lmfitmod$par) - length(nullfitmod$par) 
     degft <- length(tfitmod$par) - length(nullfitmod$par) 
     degfanova <- length(anovafitmod$par) - length(nullfitmod$par) 
     degfeightanova <- length(eightanovafitmod$par) - length(nullfitmod$par) 
     
     # test stats
    t.statreg <- 2*(nullfitmod$value - lmfitmod$value)
    t.statt <- 2*(nullfitmod$value - tfitmod$value)
    t.statanova <- 2*(nullfitmod$value - anovafitmod$value)
    t.stateightanova <- 2*(nullfitmod$value - eightanovafitmod$value)
    
    # pvalues 
    reg.p[j] <- 1-pchisq(t.statreg, df=degfreg)
    t.p[j]<- 1-pchisq(t.statt, df=degft)
    anova.p[j]<- 1-pchisq(t.statanova, df=degfanova)
    eightanova.p[j]<- 1-pchisq(t.stateightanova, df=degfeightanova)
  }
  dfp[,1]<- c(1, 2, 4, 6, 8, 12, 16, 24)
  dfp[i,2] <- mean(reg.p)
  dfp[i,3] <- mean(t.p)
  dfp[i,4] <- mean(anova.p)
  dfp[i,5] <- mean(eightanova.p)
}

```

Scatterplot with each model as a different color!
Linear model should be best

## Visualize!!

```{r}
pvalues <- read.csv("P-values.csv")
Model <- pvalues$Model
p <- ggplot(pvalues, aes(x=pvalues$Sigmas, y=pvalues$Mean_p, colour=Model)) + geom_point (size=2) + xlab("Sigma Values") + ylab("Mean p-values") 
p
     
```

The regression model performs best across the sigmas, as evidenced by it's low p-value scores. The eight level anova should perform close to the regression, but there's something wrong with our code. After that, the four level anova should perform in between the t-test and the regression, again our code is wrong. Finally the t-test, as expected, performed worse (should have). Our model is linear, so a linear regression should fit best, followed by models that break up the data into larger numbers of groups (closer to a linear model). 

It's clearly best to try to fit linear data to a linear model, but when resources are constrained, it seems the best strategy is to fit as many groups as possible. In this case, you need at least four experimental units (four way anova), to detect a significant effect. 

