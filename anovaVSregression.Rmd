<<<<<<< HEAD
---
title: "ANOVA vs Regression"
output: html_document
---

# PART 1 REGRESSION VS ANOVA

## An ANOVA-design and a regression-design experiment

```{r include=FALSE}
rm(list=ls())
setwd("/Users/emilynonnamaker/Box/personalStuff/School/PhD/biocomputingFall2018/biocomputing_StatsGroupProject")
library(ggplot2)
library(arm)
library(tidyr)
library(reshape2)
antibiotics <- read.csv("antibiotics.csv")
sugar <- read.csv("sugar.csv")

#playing around - check out the data
boxplot(growth ~ trt, data=antibiotics)
lm <- lm(growth ~ sugar, data=sugar)
aov <- aov(growth ~ trt, data=antibiotics)

# how a normal person would do this
summary(aov)
posthoc <- TukeyHSD(aov, "trt", conf.level=0.95) #a, b, ac, bd. 
posthoc
```

## Now to do the actual assingment:

```{r}
# set up your data: 

N <- length(antibiotics$growth)
y <- antibiotics$growth
x<- antibiotics$trt
antibiotics$x1 <- ifelse(x=="ab1", 1, 0) #dummy variables
antibiotics$x2 <- ifelse(x=="ab2", 1, 0) #dummy variables
antibiotics$x3 <- ifelse(x=="ab3", 1, 0) #dummy variables

#Build null model 
nullmod<-function(p,x,y){
  B0=p[1]
  sigma=exp(p[2])
  
  expected=B0

null=-sum(dnorm(x=y,mean=expected,sd=sigma, log = TRUE))
return(null)
}

#Build full model
fullmod<-function(p,x,y){
B0=p[1]
B1=p[2]
B2=p[3]
B3=p[4]
sigma=exp(p[5])

expected=B0+B1*x[,3]+B2*x[,4]+B3*x[,5]

full=-sum(dnorm(x=y,mean=expected,sd=sigma,log = TRUE))
return(full)
}

# Check fit
nullguess <- c(1, 2)
fullguess <- c(18, -12, -3, -4, 1)
fitnull=optim(par=nullguess,fn=nullmod,y=antibiotics$growth) # converges
fitfull=optim(par=fullguess,fn=fullmod,x=antibiotics,y=antibiotics$growth) # converges

# Get t.statistic and p value. 
df <- length(fitfull$par) - length(fitnull$par)
t.stat <- 2*(fitnull$value - fitfull$value)
t.stat # 37.90134

1-pchisq(t.stat, df=df) # 2.965739e-08
```

Looks like there's a significant difference between the null and full anova model (t = 25.69, p < .001), suggesting that there are differences between treatments. 

## Graphical visualization!

Using a posthoc Tukey test (see hidden code) we can find significant differences between groups, and add these differences as letters to our visualization (see graph below)

```{r}
p <- ggplot(antibiotics, aes(x=trt, y=growth, fill=trt)) + 
  geom_boxplot()
p + geom_jitter(shape=16, position=position_jitter(0.2), alpha=0.3) + theme_classic() + labs(x = "Treatment", y= "Growth") + annotate("text", x = c(1, 2, 3, 4), y = c(23, 23, 23, 23), label = c("a", "b", "ac", "bd"))
```


## Regression

Build the null model:
 
```{r}
lmnullmodreg<-function(p,x,y){
  B0=p[1]
  sigma=exp(p[2])
  
  expected=B0

null=-sum(dnorm(x=y,mean=expected,sd=sigma, log = TRUE))
return(null)
}
```

Now build the extension:

```{r}
lmfullmodreg<-function(p,x,y){
  B0=p[1]
  B1=p[2]
  sigma=exp(p[3])
  
  expected=B0+B1*x

  null=-sum(dnorm(x=y,mean=expected,sd=sigma, log = TRUE))
  return(null)
}
```

Give it a guess:

```{r}
lmnullregguess <- c(1, 1)
lmfullregguess <- c(1, 2, 3)
lmfitregnull=optim(par=lmnullregguess,fn=lmnullmodreg,x=sugar$sugar,y=sugar$growth) # converges
lmfitregfull=optim(par=lmfullregguess,fn=lmfullmodreg,x=sugar$sugar,y=sugar$growth) # converges
```

Now to compare:

```{r}
t.statreg <- 2*(lmfitregnull$value - lmfitregfull$value)
t.statreg # 39.92512
dfreg <- length(lmfitregfull$par) - length(lmfitregnull$par)
1-pchisq(t.stat, df=dfreg) # 7.441429e-10, woot
```

Comparing between the null model of no effect of sugar concentration on growth to the model taking this affect into account, we can see that sugar concentration does have a significant affect on growth (t = 39.92512, p < .0001). Looking at the graph below, we can tell that sugar concentration has a positive affect, with higher concentrations of sugar increasing growth. 

## Visualization!

```{r}
a <- ggplot(data=sugar,aes(x=sugar,y=growth))
a + geom_point() + coord_cartesian() + labs( x = "sugar concentration", y = "growth") + theme_classic() + geom_smooth(method = "lm")
```
=======
---
title: "ANOVA and Regression Results"
output: html_document
---

# PART 1 REGRESSION VS ANOVA

## An ANOVA-design and a regression-design experiment

```{r include=FALSE}
rm(list=ls())
setwd("/Users/emilynonnamaker/Box/personalStuff/School/PhD/biocomputingFall2018/biocomputing_StatsGroupProject")
library(ggplot2)
library(arm)
library(tidyr)
library(reshape2)
antibiotics <- read.csv("antibiotics.csv")
sugar <- read.csv("sugar.csv")

#playing around - check out the data
boxplot(growth ~ trt, data=antibiotics)
lm <- lm(growth ~ sugar, data=sugar)
aov <- aov(growth ~ trt, data=antibiotics)

# how a normal person would do this
summary(aov)
posthoc <- TukeyHSD(aov, "trt", conf.level=0.95) #a, b, ac, bd. 
posthoc
```

## Now to do the actual assingment:

```{r}
# set up your data: 

N <- length(antibiotics$growth)
y <- antibiotics$growth
x<- antibiotics$trt
antibiotics$x1 <- ifelse(x=="ab1", 1, 0) #dummy variables
antibiotics$x2 <- ifelse(x=="ab2", 1, 0) #dummy variables
antibiotics$x3 <- ifelse(x=="ab3", 1, 0) #dummy variables

#Build null model 
nullmod<-function(p,x,y){
  B0=p[1]
  sigma=exp(p[2])
  
  expected=B0

null=-sum(dnorm(x=y,mean=expected,sd=sigma, log = TRUE))
return(null)
}

#Build full model
fullmod<-function(p,x,y){
B0=p[1]
B1=p[2]
B2=p[3]
B3=p[4]
sigma=exp(p[5])

expected=B0+B1*x[,3]+B2*x[,4]+B3*x[,5]

full=-sum(dnorm(x=y,mean=expected,sd=sigma,log = TRUE))
return(full)
}

# Check fit
nullguess <- c(1, 2)
fullguess <- c(18, -12, -3, -4, 1)
fitnull=optim(par=nullguess,fn=nullmod,y=antibiotics$growth) # converges
fitfull=optim(par=fullguess,fn=fullmod,x=antibiotics,y=antibiotics$growth) # converges

# Get t.statistic and p value. 
df <- length(fitfull$par) - length(fitnull$par)
t.stat <- 2*(fitnull$value - fitfull$value)
t.stat # 37.90134

1-pchisq(t.stat, df=df) # 2.965739e-08
```

Looks like there's a significant difference between the null and full anova model (t = 25.69, p < .001), suggesting that there are differences between treatments. 

## Graphical visualization!

Using a posthoc Tukey test (see hidden code) we can find significant differences between groups, and add these differences as letters to our visualization (see graph below)

```{r}
p <- ggplot(antibiotics, aes(x=trt, y=growth, fill=trt)) + 
  geom_boxplot()
p + geom_jitter(shape=16, position=position_jitter(0.2), alpha=0.3) + theme_classic() + labs(x = "Treatment", y= "Growth") + annotate("text", x = c(1, 2, 3, 4), y = c(23, 23, 23, 23), label = c("a", "b", "ac", "bd"))
```

## Regression

Build the null model:
 
```{r}
lmnullmodreg<-function(p,x,y){
  B0=p[1]
  sigma=exp(p[2])
  
  expected=B0

null=-sum(dnorm(x=y,mean=expected,sd=sigma, log = TRUE))
return(null)
}
```

Now build the extension:

```{r}
lmfullmodreg<-function(p,x,y){
  B0=p[1]
  B1=p[2]
  sigma=exp(p[3])
  
  expected=B0+B1*x

  null=-sum(dnorm(x=y,mean=expected,sd=sigma, log = TRUE))
  return(null)
}
```

Give it a guess:

```{r}
lmnullregguess <- c(1, 1)
lmfullregguess <- c(1, 2, 3)
lmfitregnull=optim(par=lmnullregguess,fn=lmnullmodreg,x=sugar$sugar,y=sugar$growth) # converges
lmfitregfull=optim(par=lmfullregguess,fn=lmfullmodreg,x=sugar$sugar,y=sugar$growth) # converges
```

Now to compare:

```{r}
t.statreg <- 2*(lmfitregnull$value - lmfitregfull$value)
t.statreg # 39.92512
dfreg <- length(lmfitregfull$par) - length(lmfitregnull$par)
1-pchisq(t.stat, df=dfreg) # 7.441429e-10, woot
```

Comparing between the null model of no effect of sugar concentration on growth to the model taking this affect into account, we can see that sugar concentration does have a significant affect on growth (t = 39.92512, p < .0001). Looking at the graph below, we can tell that sugar concentration has a positive affect, with higher concentrations of sugar increasing growth. 

## Visualization!

```{r}
a <- ggplot(data=sugar,aes(x=sugar,y=growth))
a + geom_point() + coord_cartesian() + labs( x = "sugar concentration", y = "growth") + theme_classic() + geom_smooth(method = "lm")
```

# Part 2 Statistical Power Analysis

```{r}
# Null model
nullmod<- function(p,x,y){
  B0=p[1]
  sigma=exp(p[2])
  
  expected=B0
  
  null=-sum(dnorm(x=y,mean=expected,sd=sigma, log = TRUE))
  return(null)
}

# Regression model
lin<-function(p,x,y){
  B0=p[1]
  B1=p[2]
  sigma=exp(p[3])
  
  expected=B0+B1*x

  lmmod=-sum(dnorm(x=y,mean=expected,sd=sigma, log = TRUE))
  return(lmmod)
}

# T-test model
ttest<-function(p,x,y){
  B0=p[1]
  B1=p[2]
  sigma=exp(p[3])
  x1=x
  expected=B0+B1*x1

  tmod=-sum(dnorm(x=y,mean=expected,sd=sigma, log = TRUE))
  return(tmod)
}

# ANOVA - four groups
anovamod<-function(p,x,y){
B0=p[1]
B1=p[2]
B2=p[3]
B3=p[4]
sigma=exp(p[5])

expected=B0+B1*x[,1]+B2*x[,2]+B3*x[,3]

anova=-sum(dnorm(x=y,mean=expected,sd=sigma,log = TRUE))
return(anova)
}

# Now an ANOVA with 8 groups
eightanovamod<-function(p,x,y){
  B0=p[1]
  B1=p[2]
  B2=p[3]
  B3=p[4]
  B4=p[5]
  B5=p[6]
  B6=p[7]
  B7=p[8]
  sigma=exp(p[9])
  
  expected=B0+B1*x[,1]+B2*x[,2]+B3*x[,3]+B4*x[,4]+B5*x[,5]+B6*x[,6]+B7*x[,7]
  
  eightanovamod=-sum(dnorm(x=y,mean=expected,sd=sigma, log = TRUE))
  return(eightanovamod)
}

```

## Now do your for loops!

```{r}
# Set up an empty data frame in which to store your p-values for each sigma and model.
dfp <- setNames(data.frame(matrix(ncol = 5, nrow = 8)), c("Sigmas", "Regression", "T.test", "Anova",  "Eight.Anova"))

# Set up your vector of sigmas to loop through
sigmas <- c(1,2,4,6,8,12,16,24)

# Write your nested for loop:
for (i in 1:length(sigmas)){ # sigma runs
  # set up your vectors for your p-values from each monte-carlo run. 
  reg.p <- numeric(10) # changes each time you use a new sigma
  t.p <- numeric(10)
  anova.p <- numeric(10)
  eightanova.p <- numeric(10)
  for (j in 1:10){ # monte carlo runs
    x <- runif(24, 0, 50) # build set of x's
    e <- rnorm(24, 0, sd = sigmas[i]) # put in your error
    y <- 10 + 0.4*x + e # write your linear equation
    df <- cbind(x, y) # bind x and y into a data frame
    df <- as.data.frame(df)
    df <- df[order(df$x),] # order for later sorting. 
    
    # dummy coding:
    
    # For t-test
    tdummy <- matrix(0, 24, 1)
    tdummy[13:24,1] <- 1
  
    # For anova
    dummy <- matrix(0, 24, 3)
    dummy[7:12,1] <- 1
    dummy[13:18,2] <-1
    dummy[19:24,3] <-1
    
    #for eight-level anova
    edummy <- matrix(0, 24, 7)
    edummy[4:6,1]<-1
    edummy[7:9,2]<-1
    edummy[10:12,3]<-1
    edummy[13:15,4]<-1
    edummy[16:18,5]<-1
    edummy[19:21,6]<-1
    edummy[22:24,7]<-1
    
    # guesses
    nullmodguess <- c(1,2) # 2
    lmmodguess <- c(1, 2, 3) # 3 
    tmodguess<-c(1,1,1,1,1) # 5
    aovguess<- c(9,10,-6,8,2) # 5 
    eightaovguess<-c(1, 12.5,15,17.5,20,22.5,25,27.5,30) # 9
    
    #optims
    nullfitmod=optim(par=nullmodguess,fn=nullmod,x=df$x,y=df$y) # converges
    lmfitmod=optim(par=lmmodguess,fn=lin,x=df$x,y=df$y) # converges 
    tfitmod=optim(par=tmodguess,fn=ttest,x=tdummy,y=df$y) # converges
    anovafitmod=optim(par=aovguess,fn=anovamod,x=dummy,y=df$y, control=list(maxit=1e5)) # converges
    eightanovafitmod=optim(par=eightaovguess,fn=eightanovamod,x=edummy,y=df$y, control=list(maxit=1e5)) # converges
   
    # degrees of freedom 
     degfreg <- length(lmfitmod$par) - length(nullfitmod$par) 
     degft <- length(tfitmod$par) - length(nullfitmod$par) 
     degfanova <- length(anovafitmod$par) - length(nullfitmod$par) 
     degfeightanova <- length(eightanovafitmod$par) - length(nullfitmod$par) 
     
     # test stats
    t.statreg <- 2*(nullfitmod$value - lmfitmod$value)
    t.statt <- 2*(nullfitmod$value - tfitmod$value)
    t.statanova <- 2*(nullfitmod$value - anovafitmod$value)
    t.stateightanova <- 2*(nullfitmod$value - eightanovafitmod$value)
    
    # pvalues 
    reg.p[j] <- 1-pchisq(t.statreg, df=degfreg)
    t.p[j]<- 1-pchisq(t.statt, df=degft)
    anova.p[j]<- 1-pchisq(t.statanova, df=degfanova)
    eightanova.p[j]<- 1-pchisq(t.stateightanova, df=degfeightanova)
  }
  dfp[,1]<- c(1, 2, 4, 6, 8, 12, 16, 24)
  dfp[i,2] <- mean(reg.p)
  dfp[i,3] <- mean(t.p)
  dfp[i,4] <- mean(anova.p)
  dfp[i,5] <- mean(eightanova.p)
}
```

Now reshape your dataframe of p-values into something you can plot!

```{r}
mframe <- melt(dfp)
mframe <- mframe[9:40,]
allsigmas <- c(1,  2,  4,  6,  8, 12, 16, 24, 1,  2,  4,  6,  8,12, 16, 24, 1,  2,  4,  6,  8, 12, 16, 24, 1,  2,  4,  6,  8, 12, 16, 24)
mframe$sigma <- allsigmas
```

Scatterplot with each model as a different color!
Linear model should be best

NOTE 8 LEVEL ANOVA STILL NOT WORKING

## Visualize!!

```{r}
mframe <- as.data.frame(mframe)
p <- ggplot(mframe, aes(x=mframe$sigma, y=mframe$value, colour=variable)) + geom_point (size=2) + xlab("Sigma Values") + ylab("Mean p-values") 
p
     
```

The regression model performs best across the sigmas, as evidenced by it's low p-value scores. The eight level anova should perform close to the regression, but there's something wrong with our code. The four level anova performs in between the t-test and the regression. Finally the t-test, as expected, performed worse than either the regression or the four level ANOVA (and should perform worse than the eight-level as well). This all makes sense, as our data is linearly correlated, so a linear regression should fit best, followed by models that break up the data into larger numbers of groups (closer to a linear model). 

It's clearly best to try to fit linear data to a linear model, but when resources are constrained, it seems the best strategy is to fit as many groups as possible. In this case, you need at least four experimental units (four way anova), to detect a significant effect. 



```{r, include=FALSE}
# get values for the eight level anova guess:
x <- c(6.25, 12.5, 18.72, 25, 31.25, 37.5, 43.75, 50)
y.vec <- rep(NA, 8)
for (i in 1:length(x)){
   y[i] <- 10 + 0.4*x[i]
   y.vec <- y
}
```
>>>>>>> e26459b27092a0a44ebda487ebe1b16258afb0bb
