---
title: "ANOVA and Regression Results"
output: html_notebook
---

# PART 1 REGRESSION VS ANOVA

This project will focus on the difference between linear models that use categorical or continuous predictor variables. When designing experiments, scientists are often limited by the number of experimental units they can use. These practical limitations may arise due to the cost of using animal subjects, the number of tanks available for aquatic mesocosm studies, or simply the number of hours in the day. This question will focus on the optimal use of these precious experimental units to increase our "statistical power" to detect the effect of
a treatment.A experimental design question that arises when experimental units are limiting is whether to distribute the units amongst some number of replicated discrete treatment levels (ANOVA-design) or spread the experimental units along a continuous gradient of treatment levels (regression-design). In this project, we
will first revisit ANOVA and regression analyses in two cases where it is clear what experimental design was chosen. We will then evaluate the ability of these two experimental designs to detect a treatment effect (i.e. their statistical power) using simulated data.

An ANOVA-design and a regression-design experiment
1) A student conducted an experiment evaluating the effect of three different new antibiotics on growth of E. coli in lab cultures. Using the data in antibiotics.txt, generate a plot that summarizes the results and test for an effect of antibiotic treatments on the growth of E. coli using an ANOVA-design linear model and
likelihood ratio test

```{r}
# Load the data:
antibiotics = read.table("antibiotics.csv", sep = ',', header = T)
antibiotics$x1 = ifelse(antibiotics[,antibiotics$trt == "ab1"],1,0 )
antibiotics$x1 = ifelse(antibiotics[antibiotics$trt == "ab1"],1,0 )


View(antibiotics)
nllike2<-function(p,x,y){ #liklihood function for quadradic equation
  #parameters:
  B0=p[1]
  B1=p[2]
  B2=p[3]
  sigma=exp(p[4])
  #deterministic model:
  expected=B0+B1*x+B2*x^2
  #calculate negative log liklihood:
  nll=-sum(dnorm(x=y,mean=expected,sd=sigma,log=TRUE))
  return(nll)
}

initialGuess2=c(1,1,1,1) #a spot to start
fit2=optim(par=initialGuess2,fn=nllike,x=data2$x, x2=(data$x)^2, y=data2$y)

print(fit2)


```

aov( ~ )
lm ( ~ )

